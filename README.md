# ðŸ§  Mechanistic Interpretability

## ðŸ“Œ Overview
In this repository, I will conduct several experiments in the field of **mechanistic interpretability of LLMs** by implementing, testing and extending different algorithms.
Mechanistic interpretability is about **reverse-engineering Transformer models** to understand which activations or components are responsible of a specific behavior under study of a model.
In other words, by applying mechanistic interpretability algorithms, we can understand which model components are essential for the model to correctly perform a task and which not.


This repository is the basis for my master's degree thesis study.
